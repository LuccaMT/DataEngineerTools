{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requête HTTP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un requête HTTP est une requête basée sur le protocole TCP, elle fait partie de la couche application de la couche OSI. Elle permet d'accéder aux données mise à disposition sur une adresse IP (ou url résolue par un DNS) et un port. \n",
    "\n",
    "Les deux ports les plus utilisés dans le web sont le 80 pour les sites en HTTP et le 443 pour les sites en HTTPS. HTTPS est une variable du protocole HTTP basé sur le protocole TLS.\n",
    "\n",
    "Il existe de nombreux types de requêtes selon la convention `REST`: \n",
    "- GET\n",
    "- POST\n",
    "- PUT \n",
    "- DELETE\n",
    "- UPDATE.\n",
    "\n",
    "Dans notre cas, nous allons utiliser la plupart du temps des GET et potentiellement des POST. \n",
    "- Le GET permet comme son nom l'indique de récupérer des informations en fonction de certains paramètres. \n",
    "- Le POST nécessite un envoi de données pour récupérer des données. Le body du post est, la plupart du temps, envoyé sous la forme d'un objet JSON.\n",
    "\n",
    "Ces requêtes encapsulent un certain nombre de paramètres qui permettent soient d'identifier une provenance et un utilisateur ou de réaliser différentes actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:47:38.635370Z",
     "start_time": "2024-10-07T19:47:38.599631Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:01.794276Z",
     "start_time": "2024-10-07T19:48:01.690378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.esiee.fr/\"\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe deux méthodes pour récupérer le contenu de la page :\n",
    "\n",
    "- `response.text` qui permet de retourner le texte sous la forme d'une chaine de charactères.\n",
    "- `response.content` qui permet de récupérer le contenu de la page sous la forme de bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:06.901620Z",
     "start_time": "2024-10-07T19:48:06.898643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:13.168682Z",
     "start_time": "2024-10-07T19:48:13.166007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer les 1000 premiers charactères de la page :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:15.251081Z",
     "start_time": "2024-10-07T19:48:15.248157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"fr-FR\">\\n<head>\\n\\n<meta charset=\"utf-8\">\\n<!-- \\n\\tThis website is powered by TYPO3 - inspiring people to share!\\n\\tTYPO3 is a free open source Content Management Framework initially created by Kasper Skaarhoj and licensed under GNU/GPL.\\n\\tTYPO3 is copyright 1998-2025 of Kasper Skaarhoj. Extensions are copyright of their respective owners.\\n\\tInformation and contribution at https://typo3.org/\\n-->\\n\\n\\n\\n<title>ESIEE Paris, l&#039;école de l&#039;innovation technologique | ESIEE Paris</title>\\n<meta name=\"generator\" content=\"TYPO3 CMS\" />\\n<meta name=\"description\" content=\"Rejoignez ESIEE Paris, grande école d&#039;ingénieur dans les domaines des transitions numérique, énergétique et environnementale. Classée dans le groupe A, parmi les meilleures écoles d&#039;ingénieur selon le classement de l&#039;Etudiant. Habilitée par la Commission des Titres d&#039;Ingénieur (CTI). Membre de la Conférence des Grandes Ecoles (CGE). \" />\\n<meta name=\"viewport\" content=\"width=device-width'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer les headers HTTP de la réponse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:23.287126Z",
     "start_time": "2024-10-07T19:48:23.284018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Tue, 25 Nov 2025 14:12:09 GMT', 'Server': 'Apache', 'Content-Language': 'fr', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'X-UA-Compatible': 'IE=edge', 'X-Content-Type-Options': 'nosniff', 'Content-Length': '16812', 'Content-Type': 'text/html; charset=utf-8', 'X-Varnish': '569344366 569344325', 'Age': '78', 'Via': '1.1 varnish (Varnish/7.1)', 'Accept-Ranges': 'bytes', 'Connection': 'keep-alive'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut modifier les paramètres de la requête et/ou ses headers. On peut par exemple ajouter un UserAgent (identifiant de l'initiateur de la requête) et un timeout de 10 secondes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:48:38.532299Z",
     "start_time": "2024-10-07T19:48:38.452433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html lang=\"fr-FR\">\\n<head>\\n\\n<meta charset=\"utf-8\">\\n<!-- \\n\\tThis website is powered by TYPO3 - inspiring people to share!\\n\\tTYPO3 is a free open source Content Management Framework initially created by Kasper Skaarhoj and licensed under GNU/GPL.\\n\\tTYPO3 is copyright 1998-2025 of Kasper Skaarhoj. Extensions are copyright of their respective owners.\\n\\tInformation and contribution at https://typo3.org/\\n-->\\n\\n\\n\\n<title>ESIEE Paris, l&#039;\\xc3\\xa9cole de l&#039;innovation technologique | ESIEE Paris</title>\\n<meta name=\"generator\" content=\"TYPO3 CMS\" />\\n<meta name=\"description\" content=\"Rejoignez ESIEE Paris, grande \\xc3\\xa9cole d&#039;ing\\xc3\\xa9nieur dans les domaines des transitions num\\xc3\\xa9rique, \\xc3\\xa9nerg\\xc3\\xa9tique et environnementale. Class\\xc3\\xa9e dans le groupe A, parmi les meilleures \\xc3\\xa9coles d&#039;ing\\xc3\\xa9nieur selon le classement de l&#039;Etudiant. Habilit\\xc3\\xa9e par la Commission des Titres d&#039;Ing\\xc3\\xa9nieur (CTI). Membre de la Conf\\xc3\\xa9rence des Grandes Ecoles (CGE). \" />\\n<meta name=\"viewport\" content=\"width='"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers, timeout = 10)\n",
    "response.content[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1\n",
    "\n",
    "- Créer une classe Python permettant de faire des requêtes HTTP.\n",
    "- Cette classe doit utiliser toujours le même UserAgent.\n",
    "- Le TimeOut sera spécifié à chaque appelle avec une valeur par défaut.\n",
    "- Un mécanisme de retry sera mis en place de façon recursive.\n",
    "\n",
    "## Exercice 2\n",
    "\n",
    "- Faire une fonction permettant de supprimer tous les espaces supperflus d'une string\n",
    "- Faire une fonction qui prend une string html et renvois une string intelligible (enlever les caractères spéciaux,\n",
    "- Récupérer le domaine en fonction d'un url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"fr-FR\">\n",
      "<head>\n",
      "\n",
      "<meta charset=\"utf-8\">\n",
      "<!-- \n",
      "\tThis website is powered by TYPO3 - inspiring people to share!\n",
      "\tTYPO3 is a free open source Content Management Framework initia\n"
     ]
    }
   ],
   "source": [
    "#Exercice 1\n",
    "\n",
    "class RequestHTTP:\n",
    "    USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'User-Agent': self.USER_AGENT})\n",
    "    \n",
    "    def get(self, url: str, timeout: float = 10.0, retries: int = 3):\n",
    "        \"\"\"\n",
    "        Requête GET avec timeout par défaut et retry récursif.\n",
    "        Lève l'exception finale si tous les retries échouent.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            resp = self.session.get(url, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            return resp\n",
    "        except requests.RequestException:\n",
    "            if retries > 0:\n",
    "                return self.get(url, timeout=timeout, retries=retries - 1)\n",
    "            raise\n",
    "    def get_text(self, url: str, timeout: float = 10.0, retries: int = 3) -> str:\n",
    "        \"\"\"Retourne le contenu texte de la page.\"\"\"\n",
    "        return self.get(url, timeout=timeout, retries=retries).text\n",
    "    \n",
    "    def get_json(self, url: str, timeout: float = 10.0, retries: int = 3):\n",
    "        \"\"\"Retourne JSON si possible, sinon None.\"\"\"\n",
    "        \n",
    "        resp = self.get(url, timeout=timeout, retries=retries)\n",
    "        try:\n",
    "            return resp.json()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "# Exemple rapide d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    client = RequestHTTP()\n",
    "    try:\n",
    "        print(client.get_text(url, timeout=5, retries=2)[:200])\n",
    "    except Exception as e:\n",
    "        print(\"Erreur:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalize_spaces: Ceci est une phrase avec des espaces\n",
      "html_to_text: Test Page Salut Bonjour & bienvenue !\n",
      "get_domain: esiee.fr\n"
     ]
    }
   ],
   "source": [
    "#Exercice 2\n",
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "import urllib.parse\n",
    "\n",
    "def normalize_spaces(s: str) -> str:\n",
    "    \"\"\"Supprime les espaces superflus et normalise les retours à la ligne/tab.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "def html_to_text(html_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforme une string HTML en texte 'intelligible' sans balises ni entités.\n",
    "    - supprime <script> et <style>\n",
    "    - enlève les balises HTML\n",
    "    - décode les entités HTML (&nbsp;, &amp;, ...)\n",
    "    - normalise unicode et supprime caractères non imprimables\n",
    "    - compresse les espaces\n",
    "    \"\"\"\n",
    "    # retirer scripts/styles\n",
    "    cleaned = re.sub(r'(?is)<(script|style).*?>.*?</\\1>', ' ', html_str)\n",
    "    # retirer balises\n",
    "    cleaned = re.sub(r'<[^>]+>', ' ', cleaned)\n",
    "    # décoder entités HTML\n",
    "    cleaned = html.unescape(cleaned)\n",
    "    # normaliser unicode\n",
    "    cleaned = unicodedata.normalize('NFKC', cleaned)\n",
    "    # supprimer retours multiples/tab et garder imprimables\n",
    "    cleaned = re.sub(r'[\\r\\n\\t]+', ' ', cleaned)\n",
    "    cleaned = ''.join(ch for ch in cleaned if ch.isprintable())\n",
    "    # enlever espaces superflus\n",
    "    return normalize_spaces(cleaned)\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Retourne le domaine principal d'une URL (sans www. et sans port).\n",
    "    Accepte aussi 'example.com' sans schéma.\n",
    "    \"\"\"\n",
    "    if '://' not in url:\n",
    "        url = 'http://' + url\n",
    "    parsed = urllib.parse.urlparse(url)\n",
    "    domain = parsed.netloc.split(':')[0].lower()\n",
    "    if domain.startswith('www.'):\n",
    "        domain = domain[4:]\n",
    "    return domain\n",
    "\n",
    "# Exemples rapides\n",
    "if __name__ == \"__main__\":\n",
    "    s = \"   Ceci   est   une   phrase \\n avec\\tdes  espaces   \"\n",
    "    print(\"normalize_spaces:\", normalize_spaces(s))\n",
    "\n",
    "    html_sample = \"<html><head><title>Test&nbsp;Page</title><style>p{}</style></head><body><h1>Salut</h1><p>Bonjour &amp; bienvenue&nbsp;!</p><script>alert(1)</script></body></html>\"\n",
    "    print(\"html_to_text:\", html_to_text(html_sample))\n",
    "\n",
    "    print(\"get_domain:\", get_domain(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation du HTML  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, il faut récupérer le code HTML d'un site web à partir d'une requête. Lorsque vous avez récupéré le texte d'un site il faut le parser. Pour cela, on utilise BeautifulSoup qui permet de transformer la structure HTML en objet Python. Cela permet de récupérer efficacement les données qui nous intéresse.\n",
    "\n",
    "Pour les webmasters, le blocage le plus souvent mis en place et un blocage sur le User-Agent. Le User-Agent est un paramètre intégré dans la requête HTTP réalisé par le Navigateur pour envoyer au front des informations basiques :\n",
    "\n",
    "- la version du Navigateur,\n",
    "- la version de l'OS\n",
    "- Le type de gestionnaire graphique (Gecko)\n",
    "- le type de device utilisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de User Agent :  \n",
    "\n",
    "`Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons à utiliser `BeautifulSoup`, il est normalement déjà installé, le cas échéant executez les lignes suivantes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:10.823860Z",
     "start_time": "2024-10-07T19:50:10.764935Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour transformer une requête (requests) en objet BeautifulSoup :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:13.979141Z",
     "start_time": "2024-10-07T19:50:13.888552Z"
    }
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour trouver tous les liens d'une page, on récupère la balise `a` qui permet de gérer les liens en HTML :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:19.065788Z",
     "start_time": "2024-10-07T19:50:19.063429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/#content\">Aller au contenu</a>,\n",
       " <a href=\"/#menu\">Aller au menu</a>,\n",
       " <a href=\"/plan-du-site/\">Plan du site</a>,\n",
       " <a href=\"/actualites/journees-portes-ouvertes-2025-2026\" target=\"_blank\" title=\"Ouvre une nouvelle fenêtre\">Journée portes ouvertes le 6 décembre de 13h à 18h. Inscrivez-vous dès maintenant !</a>,\n",
       " <a href=\"/\"><img alt=\"ESIEE PARIS\" class=\"a42-ac-replace-img\" src=\"/typo3conf/ext/esiee_sitepackage/Resources/Public/imgs/svg/logo-esiee.svg\"/></a>,\n",
       " <a href=\"/brochures-1\">Brochures</a>,\n",
       " <a href=\"/informations/etudiantes-et-etudiants\">Espace élèves</a>,\n",
       " <a href=\"/\" hreflang=\"fr-FR\" title=\"Français\">\n",
       " <span>Fr</span>\n",
       " </a>,\n",
       " <a href=\"/en/\" hreflang=\"en-US\" title=\"English\">\n",
       " <span>En</span>\n",
       " </a>,\n",
       " <a href=\"/candidater-1\">Candidater</a>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"a\")[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi préciser la classe HTML qu'on veut récupérer :\n",
    "\n",
    "```python\n",
    "soup.find_all(class_=\"<CLASS_NAME>\")[0:10]\n",
    "```\n",
    "\n",
    "Ici par exemple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:34.406680Z",
     "start_time": "2024-10-07T19:50:34.400091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<button aria-controls=\"searchbox-header-form\" aria-expanded=\"false\" class=\"toggler\">\n",
       " <i class=\"fa-solid fa-magnifying-glass\"></i>\n",
       " <i class=\"fa-solid fa-xmark\"></i>\n",
       " <span class=\"sr-only\">\n",
       " <span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> la recherche\n",
       " \t\t</span>\n",
       " </button>,\n",
       " <button aria-controls=\"submenu-40\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu : </span>L'école</button>,\n",
       " <button aria-controls=\"submenu-563\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu : </span>Gouvernance et conseils</button>,\n",
       " <button aria-controls=\"submenu-65\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu : </span>Départements d'enseignements et de recherche</button>,\n",
       " <button aria-controls=\"submenu-67\" aria-expanded=\"false\" class=\"toggler\"><span class=\"sr-only\"><span class=\"display\">Afficher</span><span class=\"hide\">Masquer</span> le sous menu : </span>Salles blanches</button>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_=\"toggler\")[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer le text sans les balises HTML :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T19:50:43.103789Z",
     "start_time": "2024-10-07T19:50:43.100752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\nESIEE Paris, l'école de l'innovation technologique | ESIEE Paris\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAller au contenu\\nAller au menu\\nPlan du site\\n\\n\\n\\n\\n\\n\\n\\nJournée portes ouvertes le 6 décembre de 13h à 18h. Inscrivez-vous dès maintenant !\\n\\n\\n\\n\\n\\nMasquer l'alerte\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBrochuresEspace élèves\\n\\n\\n\\nFr\\n\\n\\n\\n\\nEn\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAfficherMasquer la recherche\\r\\n\\t\\t\\n\\n\\n\\nSaisissez votre recherche\\xa0:\\n\\nLancer la recherche\\n\\n\\n\\nCandidater\\n\\nAfficherMasquer le menu\\n\\n\\n\\n\\n\\nRetour au menu principalAfficherMasquer le sous menu\\xa0: L'écolePourquoi choisir ESIEE Paris ?AfficherMasquer le sous menu\\xa0: Gouvernance et conseilsGouvernance et conseilsConseil scientifiqueAfficherMasquer le sous menu\\xa0: Départements d'enseignements et de rechercheInformatique et télécommunicationsIngénierie des systèmes cyberphysiquesIngénierie industrielleSanté, énergie et environnement durableManagement, sciences humaines et languesCorps professoralDémarche Qualité et DD&RSAfficherMasquer le sous menu\\xa0: Salles blanchesSalles blanchesÉquipements et procé\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "### Exercice 3\n",
    "\n",
    "Améliorer la classe développé précédemment.\n",
    "\n",
    "- Ajouter une méthode pour récupérer l'objet soup d'un url\n",
    "- Récupérer une liste de User Agent et effectuer une rotation aléatoire sur celui à utiliser\n",
    "- Utiliser cette classe pour parser une page HTML et récupérer : le titre, tous les H1 (si ils existent), les liens vers les images, les liens sortants vers d'autres sites, et le texte principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ESIEE Paris, l&#039;école de l&#039;innovation technologique | ESIEE Paris\n",
      "H1: ['']\n",
      "Images: ['https://www.esiee.fr/typo3conf/ext/esiee_sitepackage/Resources/Public/imgs/svg/logo-esiee.svg', 'https://www.esiee.fr/fileadmin/user_upload/Fichiers/image-home/ESIEE-Home-Main-Picture.webp', 'https://www.esiee.fr/fileadmin/user_upload/Fichiers/image-home/ESIEE-Home-Main-Picture.webp', 'https://www.esiee.fr/fileadmin/_processed_/9/5/csm_actu-jpo-1344x840_6889669208.jpg', 'https://www.esiee.fr/fileadmin/_processed_/9/5/csm_actu-jpo-1344x840_6889669208.jpg']\n",
      "External links: ['http://esiee.jobteaser.com/fr/recruiter_account/job_offers', 'https://www.cge.asso.fr/', 'https://www.cti-commission.fr/', 'https://www.cti-commission.fr/la-cti/demarche-qualite/systeme-qe/eur-ace', 'https://www.enseignementsup-recherche.gouv.fr/fr/labels-des-formations-controlees-par-l-etat-46088']\n",
      "Text snippet: ESIEE Paris, l'école de l'innovation technologique | ESIEE Paris Aller au contenu Aller au menu Plan du site Journée portes ouvertes le 6 décembre de 13h à 18h. Inscrivez-vous dès maintenant ! Masquer\n"
     ]
    }
   ],
   "source": [
    "#Exercice 3\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup  # optional\n",
    "    _HAS_BS = True\n",
    "except Exception:\n",
    "    _HAS_BS = False\n",
    "\n",
    "class RequestHTTP:\n",
    "    USER_AGENTS = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\"User-Agent\": self.USER_AGENTS[0]})\n",
    "\n",
    "    def _set_random_ua(self):\n",
    "        ua = random.choice(self.USER_AGENTS)\n",
    "        self.session.headers.update({\"User-Agent\": ua})\n",
    "        return ua\n",
    "\n",
    "    def get(self, url: str, timeout: float = 10.0, retries: int = 3, rotate_ua: bool = True, backoff: float = 0.5):\n",
    "        \"\"\"\n",
    "        GET with optional UA rotation and recursive retry with simple backoff.\n",
    "        \"\"\"\n",
    "        if rotate_ua:\n",
    "            self._set_random_ua()\n",
    "        try:\n",
    "            resp = self.session.get(url, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            return resp\n",
    "        except requests.RequestException:\n",
    "            if retries > 0:\n",
    "                time.sleep(backoff)\n",
    "                return self.get(url, timeout=timeout, retries=retries - 1, rotate_ua=rotate_ua, backoff=backoff * 2)\n",
    "            raise\n",
    "\n",
    "    def get_text(self, url: str, timeout: float = 10.0, retries: int = 3, rotate_ua: bool = True) -> str:\n",
    "        return self.get(url, timeout=timeout, retries=retries, rotate_ua=rotate_ua).text\n",
    "\n",
    "    def get_soup(self, url: str, timeout: float = 10.0, retries: int = 3, rotate_ua: bool = True):\n",
    "        \"\"\"\n",
    "        Retourne un objet BeautifulSoup si bs4 est installé, sinon retourne le HTML brut.\n",
    "        \"\"\"\n",
    "        reponse = self.get(url, timeout=timeout, retries=retries, rotate_ua=rotate_ua)\n",
    "        if _HAS_BS:\n",
    "            url_soup = BeautifulSoup(reponse).text\n",
    "        return url_soup\n",
    "    \n",
    "    def parse_page(self, url: str, timeout: float = 10.0, retries: int = 3, rotate_ua: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Parse la page et renvoie:\n",
    "          - title (str)\n",
    "          - h1 (list[str])\n",
    "          - images (list[str]) absolute URLs\n",
    "          - external_links (list[str]) absolute URLs to other domains\n",
    "          - text_main (str) cleaned text (uses html_to_text from Exercice 2)\n",
    "        \"\"\"\n",
    "        html_text = self.get_text(url, timeout=timeout, retries=retries, rotate_ua=rotate_ua)\n",
    "        base_domain = get_domain(url)\n",
    "\n",
    "        # title\n",
    "        m = re.search(r'(?is)<title[^>]*>(.*?)</title>', html_text)\n",
    "        title = (m.group(1).strip() if m else \"\").replace(\"\\n\", \" \")\n",
    "\n",
    "        # all H1\n",
    "        raw_h1 = re.findall(r'(?is)<h1[^>]*>(.*?)</h1>', html_text)\n",
    "        h1_list = [re.sub(r'<[^>]+>', '', h).strip() for h in raw_h1]\n",
    "\n",
    "        # images src\n",
    "        imgs = re.findall(r'(?is)<img[^>]+src=[\"\\']?([^\"\\'>\\s]+)', html_text)\n",
    "        imgs_abs = [urllib.parse.urljoin(url, src) for src in imgs]\n",
    "\n",
    "        # all links, make absolute\n",
    "        links = re.findall(r'(?is)<a[^>]+href=[\"\\']?([^\"\\'>\\s]+)', html_text)\n",
    "        links_abs = [urllib.parse.urljoin(url, href) for href in links]\n",
    "        # external links\n",
    "        external_links = [l for l in links_abs if l.startswith('http') and get_domain(l) != base_domain]\n",
    "\n",
    "        # main text cleaned using html_to_text from Exercice 2 (assumes present)\n",
    "        try:\n",
    "            text_main = html_to_text(html_text)\n",
    "        except NameError:\n",
    "            # fallback: strip tags and normalize spaces\n",
    "            text_main = re.sub(r'<[^>]+>', ' ', html_text)\n",
    "            text_main = re.sub(r'\\s+', ' ', text_main).strip()\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"h1\": h1_list,\n",
    "            \"images\": imgs_abs,\n",
    "            \"external_links\": external_links,\n",
    "            \"text_main\": text_main\n",
    "        }\n",
    "\n",
    "# Exemple d'utilisation rapide\n",
    "if __name__ == \"__main__\":\n",
    "    client = RequestHTTP()\n",
    "    page = client.parse_page(\"https://www.esiee.fr/\", timeout=5, retries=2, rotate_ua=True)\n",
    "    print(\"Title:\", page[\"title\"])\n",
    "    print(\"H1:\", page[\"h1\"])\n",
    "    print(\"Images:\", page[\"images\"][:5])\n",
    "    print(\"External links:\", page[\"external_links\"][:5])\n",
    "    print(\"Text snippet:\", page[\"text_main\"][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation des appels d'API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Losque le front du site récupère des données sur une API gérée par le back, un appel d'API est réalisé. Cet appel est recensé dans les appels réseaux. Il est alors possible de re-jouer cet appel pour récupérer à nouveau les données. Il est très facile de récupérer ces appels dans l'onglet Network de la console développeur de Chrome ou FireFox. La console vous permet de copier le code CURL de la requête et vous pouvez ensuite la transformer en code Python depuis le site https://curl.trillworks.com/.\n",
    "\n",
    "Souvent les APIs sont bloquées avec certains paramètres. L'API vérifie que dans les headers de la requête HTTP ces paramètres sont présents :\n",
    "* un token généré à la volée avec des protocoles OAuth2 (ou moins développés).\n",
    "* un referer provenant du site web (la source de la requête), très facile à falsifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice \n",
    "### Exercice 4\n",
    "\n",
    "- Utiliser les informations développées plus haut pour récupérer les premiers résultats d'une recherche d'une requête\n",
    "sur Google. \n",
    "\n",
    "Tips : \n",
    "\n",
    "- Ouvrir les outils de développements de Chrome ou Firefox\n",
    "- Onglet Network\n",
    "- Fouiller dans les requêtes pour voir à quoi ressemble un appel API Google\n",
    "- Utilisez beautiful soup pour convertir le contenu de la request en objet et accéder aux balises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 200, HTML length: 84641\n",
      "Aucun résultat trouvé - Google bloque probablement le scraping\n",
      "Extrait HTML: <!DOCTYPE html><html lang=\"fr\"><head><title>Google Search</title><style>body{background-color:var(--xhUGwc)}</style><script nonce=\"7chm09Gvyhi7mZqAhN5KxQ\">window.google = window.google || {};window.google.c = window.google.c || {cap:0};</script></head><body><noscript><style>table,div,span,p{display:none}</style><meta content=\"0;url=/httpservice/retry/enablejs?sei=g7olaYDQD-itkdUP84K10A0\" http-equiv=\"refresh\"><div style=\"display:block\">Cliquez <a href=\"/httpservice/retry/enablejs?sei=g7olaYDQD-it\n"
     ]
    }
   ],
   "source": [
    "#Exercice 4\n",
    "import urllib.parse\n",
    "import html\n",
    "\n",
    "def search_google(query: str, num: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Récupère les premiers résultats Google pour `query`.\n",
    "    Note: Google bloque souvent le scraping. Utilisez l'API officielle pour la production.\n",
    "    \"\"\"\n",
    "    client = RequestHTTP()\n",
    "    client.session.headers.update({\n",
    "        \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "    })\n",
    "    base = \"https://www.google.com/search\"\n",
    "    url = base + \"?\" + urllib.parse.urlencode({\"q\": query, \"num\": str(num), \"hl\": \"fr\"})\n",
    "    \n",
    "    try:\n",
    "        resp = client.get(url, timeout=10, retries=2, rotate_ua=True)\n",
    "        html_text = resp.text\n",
    "        print(f\"Status: {resp.status_code}, HTML length: {len(html_text)}\")\n",
    "        \n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        results = []\n",
    "        \n",
    "        # Chercher les conteneurs de résultat (div.yuRUbf > a pour desktop)\n",
    "        for a in soup.select(\"div.yuRUbf > a\"):\n",
    "            h3 = a.find(\"h3\")\n",
    "            if not h3:\n",
    "                continue\n",
    "            title = h3.get_text(\" \", strip=True)\n",
    "            url_result = a.get(\"href\", \"\")\n",
    "            \n",
    "            # Récupérer le snippet (description)\n",
    "            snippet = \"\"\n",
    "            parent = a.find_parent()\n",
    "            if parent:\n",
    "                snippet_div = parent.find_next(\"div\", class_=lambda c: c and any(cls in str(c) for cls in [\"VwiC3b\", \"s\"]))\n",
    "                if snippet_div:\n",
    "                    snippet = snippet_div.get_text(\" \", strip=True)\n",
    "            \n",
    "            results.append({\"title\": title, \"url\": url_result, \"snippet\": snippet})\n",
    "            if len(results) >= num:\n",
    "                break\n",
    "        \n",
    "        if not results:\n",
    "            print(\"Aucun résultat trouvé - Google bloque probablement le scraping\")\n",
    "            print(\"Extrait HTML:\", html_text[:500])\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la recherche: {e}\")\n",
    "        return []\n",
    "\n",
    "# Exemple d'utilisation\n",
    "results = search_google(\"ESIEE Paris\", num=5)\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. {r['title']}\")\n",
    "    print(f\"   URL: {r['url']}\")\n",
    "    if r['snippet']:\n",
    "        print(f\"   Snippet: {r['snippet'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice Final  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice Final\n",
    "Utilisez tout ce que vous avez appris pour récupérer des articles de News avec une catégorie. Il est souvent intéressant de partir des flux RSS pour commencer :\n",
    "\n",
    "Les données doivent comprendre :\n",
    "- Le texte important propre\n",
    "- L'url\n",
    "- Le domaine\n",
    "- la catégorie\n",
    "- Le titre de l'article\n",
    "- Le titre de la page\n",
    "- (Facultatif) : les images\n",
    "\n",
    "Tips : \n",
    "\n",
    "- Taper le nom de votre média favoris + RSS (par exemple : https://www.lemonde.fr/rss/)\n",
    "- Aller dans le DOM de la page \n",
    "- Trouver les catégories et les liens vers les articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récupération des articles depuis https://www.lemonde.fr/rss/une.xml...\n",
      "\n",
      "Erreur lors du scraping de https://www.lemonde.fr/international/article/2025/11/25/plan-americain-pour-l-ukraine-la-russie-utilise-la-diplomatie-comme-une-arme-de-guerre_6654706_3210.html: object of type 'Response' has no len()\n",
      "Erreur lors du scraping de https://www.lemonde.fr/economie/article/2025/11/25/auchan-veut-basculer-tous-ses-supermarches-sous-banniere-intermarche-et-netto_6654749_3234.html: object of type 'Response' has no len()\n",
      "Erreur lors du scraping de https://www.lemonde.fr/international/article/2025/11/25/plan-americain-pour-l-ukraine-la-russie-utilise-la-diplomatie-comme-une-arme-de-guerre_6654706_3210.html: object of type 'Response' has no len()\n",
      "Erreur lors du scraping de https://www.lemonde.fr/economie/article/2025/11/25/auchan-veut-basculer-tous-ses-supermarches-sous-banniere-intermarche-et-netto_6654749_3234.html: object of type 'Response' has no len()\n",
      "Erreur lors du scraping de https://www.lemonde.fr/culture/article/2025/11/25/cambriolage-au-louvre-quatre-personnes-supplementaires-interpellees_6654768_3246.html: object of type 'Response' has no len()\n",
      "Erreur lors du scraping de https://www.lemonde.fr/culture/article/2025/11/25/cambriolage-au-louvre-quatre-personnes-supplementaires-interpellees_6654768_3246.html: object of type 'Response' has no len()\n",
      "Erreur lors du scraping de https://www.lemonde.fr/les-decodeurs/article/2025/11/25/budget-ces-mesures-fortes-sur-lesquelles-les-deputes-s-etaient-accordes_6654747_4355770.html: object of type 'Response' has no len()\n",
      "Erreur lors du scraping de https://www.lemonde.fr/les-decodeurs/article/2025/11/25/budget-ces-mesures-fortes-sur-lesquelles-les-deputes-s-etaient-accordes_6654747_4355770.html: object of type 'Response' has no len()\n",
      "Erreur lors du scraping de https://www.lemonde.fr/afrique/article/2025/11/25/au-soudan-du-sud-l-aide-internationale-est-au-plus-bas-depuis-2011-oxfam-alerte-sur-une-famine-historique_6654765_3212.html: object of type 'Response' has no len()\n",
      "\n",
      "================================================================================\n",
      "Article 1\n",
      "================================================================================\n",
      "Titre: Plan américain pour l’Ukraine : « La Russie utilise la diplomatie comme une arme de guerre »\n",
      "URL: https://www.lemonde.fr/international/article/2025/11/25/plan-americain-pour-l-ukraine-la-russie-utilise-la-diplomatie-comme-une-arme-de-guerre_6654706_3210.html\n",
      "Domaine: lemonde.fr\n",
      "Catégorie: Non catégorisé\n",
      "Description: Le chercheur et journaliste Peter Pomerantsev, spécialiste de la propagande russe, décrypte, dans un entretien au « Monde », le « plan de paix » de Donald Trump....\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Article 2\n",
      "================================================================================\n",
      "Titre: Auchan veut basculer tous ses supermarchés sous bannière Intermarché et Netto\n",
      "URL: https://www.lemonde.fr/economie/article/2025/11/25/auchan-veut-basculer-tous-ses-supermarches-sous-banniere-intermarche-et-netto_6654749_3234.html\n",
      "Domaine: lemonde.fr\n",
      "Catégorie: Non catégorisé\n",
      "Description: Ce projet, qui pourrait aboutir à la fin de l’année 2026, doit encore être discuté au sein des instances représentatives du personnel et recevoir l’aval de l’Autorité de la concurrence....\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Article 3\n",
      "================================================================================\n",
      "Titre: Cambriolage au Louvre : quatre personnes supplémentaires interpellées\n",
      "URL: https://www.lemonde.fr/culture/article/2025/11/25/cambriolage-au-louvre-quatre-personnes-supplementaires-interpellees_6654768_3246.html\n",
      "Domaine: lemonde.fr\n",
      "Catégorie: Non catégorisé\n",
      "Description: Il s’agit de deux hommes, âgés de 38 et 39 ans, et de deux femmes, âgées de 31 et 40 ans, tous originaires de région parisienne....\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Article 4\n",
      "================================================================================\n",
      "Titre: Budget : ces mesures fortes sur lesquelles les députés s’étaient accordés\n",
      "URL: https://www.lemonde.fr/les-decodeurs/article/2025/11/25/budget-ces-mesures-fortes-sur-lesquelles-les-deputes-s-etaient-accordes_6654747_4355770.html\n",
      "Domaine: lemonde.fr\n",
      "Catégorie: Non catégorisé\n",
      "Description: L’Assemblée a fini par rejeter, sans surprise et presque à l’unanimité, le budget sur lequel les députés travaillaient depuis fin octobre. Mais certaines des mesures qui avaient été votées, à la faveu...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Article 5\n",
      "================================================================================\n",
      "Titre: Au Soudan du Sud, l’aide internationale est au plus bas depuis 2011, Oxfam alerte sur une famine historique\n",
      "URL: https://www.lemonde.fr/afrique/article/2025/11/25/au-soudan-du-sud-l-aide-internationale-est-au-plus-bas-depuis-2011-oxfam-alerte-sur-une-famine-historique_6654765_3212.html\n",
      "Domaine: lemonde.fr\n",
      "Catégorie: Non catégorisé\n",
      "Description: Les coupes budgétaires des bailleurs traditionnels – notamment des Etats-Unis, le plus grand d’entre eux – ont « privé la population d’une aide humanitaire vitale au moment même où elle en avait le pl...\n",
      "\n",
      "\n",
      "Pour tester avec d'autres médias, essayez:\n",
      "- Le Figaro: https://www.lefigaro.fr/rss/figaro_actualites.xml\n",
      "- Libération: https://www.liberation.fr/arc/outboundfeeds/rss/\n",
      "- Les Echos: https://www.lesechos.fr/rss/\n",
      "Erreur lors du scraping de https://www.lemonde.fr/afrique/article/2025/11/25/au-soudan-du-sud-l-aide-internationale-est-au-plus-bas-depuis-2011-oxfam-alerte-sur-une-famine-historique_6654765_3212.html: object of type 'Response' has no len()\n",
      "\n",
      "================================================================================\n",
      "Article 1\n",
      "================================================================================\n",
      "Titre: Plan américain pour l’Ukraine : « La Russie utilise la diplomatie comme une arme de guerre »\n",
      "URL: https://www.lemonde.fr/international/article/2025/11/25/plan-americain-pour-l-ukraine-la-russie-utilise-la-diplomatie-comme-une-arme-de-guerre_6654706_3210.html\n",
      "Domaine: lemonde.fr\n",
      "Catégorie: Non catégorisé\n",
      "Description: Le chercheur et journaliste Peter Pomerantsev, spécialiste de la propagande russe, décrypte, dans un entretien au « Monde », le « plan de paix » de Donald Trump....\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Article 2\n",
      "================================================================================\n",
      "Titre: Auchan veut basculer tous ses supermarchés sous bannière Intermarché et Netto\n",
      "URL: https://www.lemonde.fr/economie/article/2025/11/25/auchan-veut-basculer-tous-ses-supermarches-sous-banniere-intermarche-et-netto_6654749_3234.html\n",
      "Domaine: lemonde.fr\n",
      "Catégorie: Non catégorisé\n",
      "Description: Ce projet, qui pourrait aboutir à la fin de l’année 2026, doit encore être discuté au sein des instances représentatives du personnel et recevoir l’aval de l’Autorité de la concurrence....\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Article 3\n",
      "================================================================================\n",
      "Titre: Cambriolage au Louvre : quatre personnes supplémentaires interpellées\n",
      "URL: https://www.lemonde.fr/culture/article/2025/11/25/cambriolage-au-louvre-quatre-personnes-supplementaires-interpellees_6654768_3246.html\n",
      "Domaine: lemonde.fr\n",
      "Catégorie: Non catégorisé\n",
      "Description: Il s’agit de deux hommes, âgés de 38 et 39 ans, et de deux femmes, âgées de 31 et 40 ans, tous originaires de région parisienne....\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Article 4\n",
      "================================================================================\n",
      "Titre: Budget : ces mesures fortes sur lesquelles les députés s’étaient accordés\n",
      "URL: https://www.lemonde.fr/les-decodeurs/article/2025/11/25/budget-ces-mesures-fortes-sur-lesquelles-les-deputes-s-etaient-accordes_6654747_4355770.html\n",
      "Domaine: lemonde.fr\n",
      "Catégorie: Non catégorisé\n",
      "Description: L’Assemblée a fini par rejeter, sans surprise et presque à l’unanimité, le budget sur lequel les députés travaillaient depuis fin octobre. Mais certaines des mesures qui avaient été votées, à la faveu...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Article 5\n",
      "================================================================================\n",
      "Titre: Au Soudan du Sud, l’aide internationale est au plus bas depuis 2011, Oxfam alerte sur une famine historique\n",
      "URL: https://www.lemonde.fr/afrique/article/2025/11/25/au-soudan-du-sud-l-aide-internationale-est-au-plus-bas-depuis-2011-oxfam-alerte-sur-une-famine-historique_6654765_3212.html\n",
      "Domaine: lemonde.fr\n",
      "Catégorie: Non catégorisé\n",
      "Description: Les coupes budgétaires des bailleurs traditionnels – notamment des Etats-Unis, le plus grand d’entre eux – ont « privé la population d’une aide humanitaire vitale au moment même où elle en avait le pl...\n",
      "\n",
      "\n",
      "Pour tester avec d'autres médias, essayez:\n",
      "- Le Figaro: https://www.lefigaro.fr/rss/figaro_actualites.xml\n",
      "- Libération: https://www.liberation.fr/arc/outboundfeeds/rss/\n",
      "- Les Echos: https://www.lesechos.fr/rss/\n"
     ]
    }
   ],
   "source": [
    "#Exercice Final (Exercice 5) - Scraper des articles de news depuis RSS\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "\n",
    "def parse_rss_feed(rss_url: str, max_articles: int = 10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse un flux RSS et récupère les articles avec leurs métadonnées.\n",
    "    \"\"\"\n",
    "    client = RequestHTTP()\n",
    "    try:\n",
    "        resp = client.get(rss_url, timeout=10, retries=2)\n",
    "        root = ET.fromstring(resp.text)\n",
    "        \n",
    "        articles = []\n",
    "        # Chercher les items RSS (max_articles premiers)\n",
    "        items = root.findall('.//item')[:max_articles]\n",
    "        \n",
    "        for item in items:\n",
    "            # Extraire les données de base du flux RSS\n",
    "            title_elem = item.find('title')\n",
    "            link_elem = item.find('link')\n",
    "            category_elem = item.find('category')\n",
    "            description_elem = item.find('description')\n",
    "            \n",
    "            title = title_elem.text if title_elem is not None else \"\"\n",
    "            link = link_elem.text if link_elem is not None else \"\"\n",
    "            category = category_elem.text if category_elem is not None else \"Non catégorisé\"\n",
    "            description = description_elem.text if description_elem is not None else \"\"\n",
    "            \n",
    "            # Nettoyer la description HTML\n",
    "            if description:\n",
    "                description = html_to_text(description)\n",
    "            \n",
    "            article_data = {\n",
    "                \"article_title\": normalize_spaces(title),\n",
    "                \"url\": link,\n",
    "                \"domain\": get_domain(link) if link else \"\",\n",
    "                \"category\": category,\n",
    "                \"description\": description,\n",
    "                \"page_title\": \"\",\n",
    "                \"text_main\": \"\",\n",
    "                \"images\": []\n",
    "            }\n",
    "            \n",
    "            # Optionnel : récupérer le contenu complet de la page\n",
    "            if link:\n",
    "                try:\n",
    "                    soup = client.get_soup(link, timeout=10, retries=1)\n",
    "                    \n",
    "                    # Titre de la page\n",
    "                    if soup.title:\n",
    "                        article_data[\"page_title\"] = soup.title.string.strip()\n",
    "                    \n",
    "                    # Texte principal (chercher dans <article> ou <main>)\n",
    "                    main_content = soup.find(['article', 'main'])\n",
    "                    if main_content:\n",
    "                        article_data[\"text_main\"] = normalize_spaces(main_content.get_text())[:500]\n",
    "                    \n",
    "                    # Images\n",
    "                    images = soup.find_all('img', limit=5)\n",
    "                    article_data[\"images\"] = [\n",
    "                        urllib.parse.urljoin(link, img.get('src', '')) \n",
    "                        for img in images if img.get('src')\n",
    "                    ]\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors du scraping de {link}: {e}\")\n",
    "            \n",
    "            articles.append(article_data)\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du parsing du flux RSS: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Exemple d'utilisation avec Le Monde\n",
    "rss_url = \"https://www.lemonde.fr/rss/une.xml\"  # Flux principal du Monde\n",
    "\n",
    "print(f\"Récupération des articles depuis {rss_url}...\\n\")\n",
    "articles = parse_rss_feed(rss_url, max_articles=5)\n",
    "\n",
    "for i, article in enumerate(articles, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Article {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Titre: {article['article_title']}\")\n",
    "    print(f\"URL: {article['url']}\")\n",
    "    print(f\"Domaine: {article['domain']}\")\n",
    "    print(f\"Catégorie: {article['category']}\")\n",
    "    print(f\"Description: {article['description'][:200]}...\")\n",
    "    if article['images']:\n",
    "        print(f\"Nombre d'images: {len(article['images'])}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nPour tester avec d'autres médias, essayez:\")\n",
    "print(\"- Le Figaro: https://www.lefigaro.fr/rss/figaro_actualites.xml\")\n",
    "print(\"- Libération: https://www.liberation.fr/arc/outboundfeeds/rss/\")\n",
    "print(\"- Les Echos: https://www.lesechos.fr/rss/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataengineertools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
